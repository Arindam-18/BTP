{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRQJMpGa4IfeBW67pygg6Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arindam-18/BTP/blob/main/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQo_ZXwPh4iE"
      },
      "outputs": [],
      "source": [
        "!pip install portalocker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import yaml\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import WikiText103\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "uZg2IzRVMGwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIMENSION = 150\n",
        "EMBEDDING_MAX_NORM = 1\n",
        "MINIMUM_FREQUENCY = 75\n",
        "MINIMUM_LENGTH = 4\n",
        "MAXIMUM_LENGTH = 256"
      ],
      "metadata": {
        "id": "VObcBBegi_F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size: int):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embeddings = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=EMBEDDING_DIMENSION,\n",
        "            max_norm=EMBEDDING_MAX_NORM,\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=EMBEDDING_DIMENSION, out_features=vocab_size\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = self.embeddings(inp)\n",
        "        x = x.mean(axis=1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KdKPpRgFjJzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        epochs,\n",
        "        train_data_loader,\n",
        "        train_steps,\n",
        "        val_data_loader,\n",
        "        val_steps,\n",
        "        checkpoint_frequency,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        model_dir,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.train_steps = train_steps\n",
        "        self.val_data_loader = val_data_loader\n",
        "        self.val_steps = val_steps\n",
        "        self.checkpoint_frequency = checkpoint_frequency\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_scheduler = LambdaLR(\n",
        "            optimizer, lr_lambda=lambda epoch: (epochs - epoch) / epochs, verbose=True\n",
        "        )\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_dir = model_dir\n",
        "\n",
        "        self.loss = {\"train\": [], \"val\": []}\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            print(\"Training...\")\n",
        "            self.train_epoch()\n",
        "            print(\"Validating...\")\n",
        "            self.val_epoch()\n",
        "            print(\n",
        "                \"Epoch: {} of {}\\nTrain Loss = {:.5f}\\nValidation Loss = {:.5f}\\n\".format(\n",
        "                    epoch + 1, self.epochs, self.loss[\"train\"][-1], self.loss[\"val\"][-1]\n",
        "                )\n",
        "            )\n",
        "            self.lr_scheduler.step()\n",
        "            if self.checkpoint_frequency:\n",
        "                self.save_checkpoint(epoch)\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = []\n",
        "        for idx, batch_data in enumerate(tqdm(self.train_data_loader), 1):\n",
        "            inputs = batch_data[0].to(self.device)\n",
        "            labels = batch_data[1].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            if idx == self.train_steps:\n",
        "                break\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "        self.loss[\"train\"].append(epoch_loss)\n",
        "\n",
        "    def val_epoch(self):\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "        with torch.no_grad():\n",
        "            for idx, batch_data in enumerate(tqdm(self.val_data_loader), 1):\n",
        "                inputs = batch_data[0].to(self.device)\n",
        "                labels = batch_data[1].to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.loss_fn(outputs, labels)\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "                if idx == self.val_steps:\n",
        "                    break\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "        self.loss[\"val\"].append(epoch_loss)\n",
        "\n",
        "    def save_checkpoint(self, epoch):\n",
        "        epoch_num = epoch + 1\n",
        "        if epoch_num % self.checkpoint_frequency == 0:\n",
        "            model_path = \"checkpoint_{}.pt\".format(str(epoch_num).zfill(3))\n",
        "            model_path = os.path.join(self.model_dir, model_path)\n",
        "            torch.save(self.model, model_path)\n",
        "\n",
        "    def save_model(self):\n",
        "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
        "        torch.save(self.model, model_path)"
      ],
      "metadata": {
        "id": "VQ0Lrod7jLw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(data_iter, tokenizer):\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        map(tokenizer, data_iter), specials=[\"<unk>\"], min_freq=MINIMUM_FREQUENCY\n",
        "    )\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def get_data_iterator(data_type):\n",
        "    data_iter = WikiText103(split=data_type)\n",
        "    data_iter = to_map_style_dataset(data_iter)\n",
        "    return data_iter\n",
        "\n",
        "\n",
        "def collate(batch, text_pipeline):\n",
        "    batch_input, batch_output = [], []\n",
        "    for text in batch:\n",
        "        token_ids = text_pipeline(text)\n",
        "\n",
        "        if len(token_ids) < MINIMUM_LENGTH * 2 + 1:\n",
        "            continue\n",
        "        if MAXIMUM_LENGTH:\n",
        "            token_ids = token_ids[:MAXIMUM_LENGTH]\n",
        "\n",
        "        for idx in range(len(token_ids) - MINIMUM_LENGTH * 2):\n",
        "            token_sequence = token_ids[idx : (idx + 1 + MINIMUM_LENGTH * 2)]\n",
        "            out = token_sequence.pop(MINIMUM_LENGTH)\n",
        "            inp = token_sequence\n",
        "            batch_input.append(inp)\n",
        "            batch_output.append(out)\n",
        "\n",
        "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "    return batch_input, batch_output\n",
        "\n",
        "\n",
        "def get_data_loader_and_vocab(data_type, batch_size, shuffle, vocab=None):\n",
        "    data_iter = get_data_iterator(data_type)\n",
        "    tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
        "    if vocab is None:\n",
        "        vocab = build_vocab(data_iter, tokenizer)\n",
        "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "    data_loader = DataLoader(\n",
        "        data_iter,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=partial(collate, text_pipeline=text_pipeline),\n",
        "    )\n",
        "    return data_loader, vocab\n",
        "\n",
        "\n",
        "def save_vocab(vocab, model_dir):\n",
        "    vocab_path = os.path.join(model_dir, \"vocab.pt\")\n",
        "    torch.save(vocab, vocab_path)\n",
        "\n",
        "\n",
        "def save_config(config: dict, model_dir: str):\n",
        "    config_path = os.path.join(model_dir, \"config.yaml\")\n",
        "    with open(config_path, \"w\") as stream:\n",
        "        yaml.dump(config, stream)\n",
        "\n",
        "\n",
        "def train(config):\n",
        "    if os.path.isdir(config[\"model_dir\"]):\n",
        "        shutil.rmtree(config[\"model_dir\"])\n",
        "    os.makedirs(config[\"model_dir\"])\n",
        "\n",
        "    train_data_loader, vocab = get_data_loader_and_vocab(\n",
        "        data_type=\"train\",\n",
        "        batch_size=config[\"train_batch_size\"],\n",
        "        shuffle=config[\"shuffle\"],\n",
        "    )\n",
        "\n",
        "    val_data_loader, _ = get_data_loader_and_vocab(\n",
        "        data_type=\"valid\",\n",
        "        batch_size=config[\"val_batch_size\"],\n",
        "        shuffle=config[\"shuffle\"],\n",
        "        vocab=vocab,\n",
        "    )\n",
        "\n",
        "    vocab_size = len(vocab.get_stoi())\n",
        "    print(f\"Vocabulary Size: {vocab_size}\\n\")\n",
        "\n",
        "    model = Word2Vec(vocab_size)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        epochs=config[\"epochs\"],\n",
        "        train_data_loader=train_data_loader,\n",
        "        train_steps=config[\"train_steps\"],\n",
        "        val_data_loader=val_data_loader,\n",
        "        val_steps=config[\"val_steps\"],\n",
        "        checkpoint_frequency=config[\"checkpoint_frequency\"],\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        model_dir=config[\"model_dir\"],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    print(\"Training Finished.\")\n",
        "    trainer.save_model()\n",
        "\n",
        "    save_vocab(vocab, config[\"model_dir\"])\n",
        "    save_config(config, config[\"model_dir\"])\n",
        "    print(\"Model artifacts saved to folder:\", config[\"model_dir\"])"
      ],
      "metadata": {
        "id": "LazJrJ7XjRJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"train_batch_size\": 64,\n",
        "    \"val_batch_size\": 64,\n",
        "    \"shuffle\": True,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"epochs\": 10,\n",
        "    \"train_steps\": None,\n",
        "    \"val_steps\": None,\n",
        "    \"checkpoint_frequency\": 2,\n",
        "    \"model_dir\": \"/content/drive/MyDrive/Word2Vec\",\n",
        "}\n",
        "\n",
        "train(config)"
      ],
      "metadata": {
        "id": "LUMOSEmOjT2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}