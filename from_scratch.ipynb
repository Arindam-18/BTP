{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMeUHcZhMgrr46w3vUjwm4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arindam-18/BTP/blob/main/from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7NyB-Dw_fNf",
        "outputId": "9d24c52c-d82f-41d7-b2ca-28335259956c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from transformers import BertModel, BertConfig\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"sample_sarcasm.csv\")\n",
        "df = pd.get_dummies(df, columns=['sarcasm'])\n",
        "text = df.iloc[:, 0]\n",
        "labels = df.iloc[:, 1:].values\n",
        "split = round(len(df) * 0.75)\n",
        "train_text = df.iloc[:split, 0]\n",
        "train_labels = df.iloc[:split, 1:].values\n",
        "val_text = df.iloc[split:, 0]\n",
        "val_labels = df.iloc[split:, 1:].values\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "EVK-8_8BC8em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = [len(i.split()) for i in text]\n",
        "max_seq_len = max(seq_len)"
      ],
      "metadata": {
        "id": "dqY2TFejFkJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token(text):\n",
        "    for sample in text:\n",
        "        yield word_tokenize(sample.lower())\n",
        "\n",
        "special_symbols = ['[PAD]', '[CLS]', '[SEP]', '[UNK]']\n",
        "vocab = build_vocab_from_iterator(token(text), min_freq=1, specials=special_symbols, special_first=True)\n",
        "vocab.set_default_index(3)"
      ],
      "metadata": {
        "id": "wM8SCL5HDI73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inputs(text):\n",
        "    seq = np.zeros((len(text), max_seq_len+2))\n",
        "    mask = np.zeros((len(text), max_seq_len+2))\n",
        "    for idx,line in enumerate(text):\n",
        "        wds = word_tokenize(line)\n",
        "        seq[idx][1:len(wds)+1] = [vocab[w] for w in wds]\n",
        "        seq[idx][0] = 1\n",
        "        seq[idx][len(wds)+1] = 2\n",
        "\n",
        "        mask[idx][0:len(wds)+2] = 1\n",
        "\n",
        "    return seq, mask"
      ],
      "metadata": {
        "id": "XGw0CEiDFhU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_train, mask_train = get_inputs(train_text)\n",
        "seq_val, mask_val = get_inputs(val_text)\n",
        "\n",
        "train_seq = torch.tensor(seq_train, dtype=torch.int)\n",
        "train_mask = torch.tensor(mask_train, dtype=torch.int)\n",
        "train_y = torch.tensor(train_labels.tolist(), dtype=torch.float)\n",
        "\n",
        "val_seq = torch.tensor(seq_val, dtype=torch.int)\n",
        "val_mask = torch.tensor(mask_val, dtype=torch.int)\n",
        "val_y = torch.tensor(val_labels.tolist(), dtype=torch.float)"
      ],
      "metadata": {
        "id": "YVzTWnlAPQ5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "b6j9viNwFhSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig(vocab_size=len(vocab), hidden_size=1536)\n",
        "bert = BertModel(config)"
      ],
      "metadata": {
        "id": "mtreB7S8Px3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(1536, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "        a = self.bert(sent_id, attention_mask=mask)\n",
        "        x = self.fc1(a[1])\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7QroKDD8Pyrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Classifier(bert)\n",
        "model = model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)\n",
        "cross_entropy  = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "2quNVDoCP1VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    total_preds = []\n",
        "    total_labels = []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        preds = model(sent_id, mask)\n",
        "        loss = cross_entropy(preds, labels)\n",
        "        total_loss = total_loss + loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "        total_preds += list(preds)\n",
        "        total_labels += labels.tolist()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "nq_q0aXCP4l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    total_preds = []\n",
        "    total_labels = []\n",
        "    for step, batch in enumerate(val_dataloader):\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(sent_id, mask)\n",
        "\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss = total_loss + loss.item()\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            preds = np.argmax(preds, axis=1)\n",
        "            total_preds += list(preds)\n",
        "            total_labels += labels.tolist()\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "RZu35ANeP7AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(filename, epoch, model, optimizer):\n",
        "    state = {\"epoch\": epoch, \"model\": model, \"optimizer\": optimizer}\n",
        "    torch.save(state, filename)"
      ],
      "metadata": {
        "id": "KzLsaH5yP9gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float(\"inf\")\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(5):\n",
        "    print(f\"Epoch {epoch+1} / 5\")\n",
        "\n",
        "    train_loss = train()\n",
        "\n",
        "    valid_loss = evaluate()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        file_name = \"topic_saved_weights.pt\"\n",
        "        save_checkpoint(file_name, epoch, model, optimizer)\n",
        "\n",
        "    print(f\"Training Loss: {train_loss}, Valid Loss: {valid_loss}\\n\")\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKhQN-9YQBTR",
        "outputId": "e1f0ea73-2fdd-4a9b-f034-328a3d175f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 5\n",
            "Training Loss: 0.9089615494012833, Valid Loss: 0.6262342408299446\n",
            "\n",
            "Epoch 2 / 5\n",
            "Training Loss: 0.6409081419308981, Valid Loss: 0.6634743511676788\n",
            "\n",
            "Epoch 3 / 5\n",
            "Training Loss: 0.6778949884076914, Valid Loss: 0.6253784224390984\n",
            "\n",
            "Epoch 4 / 5\n",
            "Training Loss: 0.6400723395248255, Valid Loss: 0.6206180527806282\n",
            "\n",
            "Epoch 5 / 5\n",
            "Training Loss: 0.630627109358708, Valid Loss: 0.6192828118801117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"topic_saved_weights.pt\"\n",
        "\n",
        "checkpoint = torch.load(path, map_location=device)\n",
        "model = checkpoint.get(\"model\")\n",
        "\n",
        "seq, mask = get_inputs(text)\n",
        "with torch.no_grad():\n",
        "    preds = model(torch.tensor(seq, dtype=torch.int).to(device), torch.tensor(mask, dtype=torch.int).to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "\n",
        "preds = np.argmax(preds, axis=1)\n",
        "\n",
        "sum = 0\n",
        "for x, y in zip(np.argmax(labels, axis=1), preds):\n",
        "    sum += x == y\n",
        "\n",
        "print((sum * 100 / len(labels)).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hppAIIt1QC_J",
        "outputId": "4b7cb12b-07c0-4bd5-fed9-967fafceb03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68.0\n"
          ]
        }
      ]
    }
  ]
}